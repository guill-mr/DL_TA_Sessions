{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83299826",
   "metadata": {},
   "source": [
    "# TA2 - CNNs on FashionMNIST (Resumable + Traceable Version)\n",
    "\n",
    "This notebook is the **resumable training** run:\n",
    "- same model architecture\n",
    "- same Optuna tuning setup\n",
    "- same training loop\n",
    "- with checkpointing, trial traceability, and resumable Optuna storage\n",
    "\n",
    "Use this version when long runs may be interrupted (cloud shutdown, runtime limits, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b0636",
   "metadata": {},
   "source": [
    "## Why a Single Validation Split (not K-fold CV)\n",
    "\n",
    "FashionMNIST already provides:\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "\n",
    "For CNN tuning with Optuna, K-fold cross-validation multiplies training cost by *K* and is usually unnecessary on a dataset this large. We use a **single stratified validation split** (90% train / 10% val), which is rigorous and much more compute-efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a0608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39232395",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DATA_ROOT = Path('session_2/data')\n",
    "CLASS_NAMES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "VAL_RATIO = 0.10\n",
    "\n",
    "MAX_EPOCHS = 15\n",
    "FINAL_MAX_EPOCHS = 20\n",
    "EARLY_STOPPING_PATIENCE = 4\n",
    "N_TRIALS = 20\n",
    "OPTUNA_TIMEOUT = None  # seconds (None = no timeout)\n",
    "\n",
    "NUM_WORKERS = min(4, os.cpu_count() or 1)\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "AMP_ENABLED = DEVICE.type == 'cuda'\n",
    "\n",
    "print(f'Device: {DEVICE}')\n",
    "print(f'AMP enabled: {AMP_ENABLED}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b951d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_MODE = 'resumable_with_checkpoints'\n",
    "ENABLE_CHECKPOINTING = True\n",
    "\n",
    "ARTIFACT_DIR = Path('session_2/artifacts/ta2_cnns_resumable')\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SPLIT_FILE = ARTIFACT_DIR / 'train_val_split.npz'\n",
    "\n",
    "STUDY_NAME = 'ta2_cnns_resumable'\n",
    "STUDY_DB = ARTIFACT_DIR / 'optuna_fashion_mnist.db'\n",
    "STUDY_STORAGE = f'sqlite:///{STUDY_DB}'\n",
    "\n",
    "print('Run mode:', RUN_MODE)\n",
    "print('Checkpointing:', ENABLE_CHECKPOINTING)\n",
    "print('Artifacts directory:', ARTIFACT_DIR.resolve())\n",
    "print('Optuna DB:', STUDY_DB.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c37919",
   "metadata": {},
   "source": [
    "## Resume Notes (for this notebook)\n",
    "\n",
    "If execution stops unexpectedly:\n",
    "1. Re-open this notebook.\n",
    "2. Re-run from the top with the same `ARTIFACT_DIR` and `STUDY_NAME`.\n",
    "3. Optuna will continue from the SQLite study.\n",
    "4. Trial/final training checkpoints in `session_2/artifacts/ta2_cnns_resumable/` let training resume with minimal loss of progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369bf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if torch.backends.cudnn.is_available():\n",
    "        torch.backends.cudnn.benchmark = DEVICE.type == 'cuda'\n",
    "\n",
    "seed_everything(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0322bc",
   "metadata": {},
   "source": [
    "## Data Loading and Raw Data Exploration\n",
    "\n",
    "We first inspect the raw grayscale images, then build dataloaders with:\n",
    "- augmentation on the train split only\n",
    "- normalization on train/val/test\n",
    "- stratified split to keep class balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e15b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = datasets.FashionMNIST(root=DATA_ROOT, train=True, download=True, transform=None)\n",
    "raw_test = datasets.FashionMNIST(root=DATA_ROOT, train=False, download=True, transform=None)\n",
    "\n",
    "print(f'Train size: {len(raw_train):,}')\n",
    "print(f'Test size: {len(raw_test):,}')\n",
    "print(f'Image shape: {raw_train[0][0].size} (grayscale)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_raw_samples(dataset, class_names, n: int = 16, seed: int = 42) -> None:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    indices = rng.choice(len(dataset), size=n, replace=False)\n",
    "\n",
    "    side = int(np.ceil(np.sqrt(n)))\n",
    "    fig, axes = plt.subplots(side, side, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        image, label = dataset[idx]\n",
    "        axes[i].imshow(np.array(image), cmap='gray')\n",
    "        axes[i].set_title(class_names[label], fontsize=9)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.suptitle('Random raw samples from FashionMNIST', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_random_raw_samples(raw_train, CLASS_NAMES, n=16, seed=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5bd671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_per_class(dataset, class_names) -> None:\n",
    "    first_indices = {}\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        if label not in first_indices:\n",
    "            first_indices[label] = idx\n",
    "        if len(first_indices) == len(class_names):\n",
    "            break\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for class_id, class_name in enumerate(class_names):\n",
    "        image, _ = dataset[first_indices[class_id]]\n",
    "        axes[class_id].imshow(np.array(image), cmap='gray')\n",
    "        axes[class_id].set_title(f'{class_id}: {class_name}')\n",
    "        axes[class_id].axis('off')\n",
    "\n",
    "    plt.suptitle('At least one raw example from each class', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_one_per_class(raw_train, CLASS_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(28, padding=2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,)),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,)),\n",
    "])\n",
    "\n",
    "\n",
    "def get_or_create_split_indices(\n",
    "    targets: np.ndarray,\n",
    "    val_ratio: float,\n",
    "    seed: int,\n",
    "    split_file: Path | None = None,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    if split_file is not None and split_file.exists():\n",
    "        data = np.load(split_file)\n",
    "        return data['train_idx'], data['val_idx']\n",
    "\n",
    "    indices = np.arange(len(targets))\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        indices,\n",
    "        test_size=val_ratio,\n",
    "        random_state=seed,\n",
    "        stratify=targets,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    train_idx = np.sort(train_idx)\n",
    "    val_idx = np.sort(val_idx)\n",
    "\n",
    "    if split_file is not None:\n",
    "        split_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        np.savez(split_file, train_idx=train_idx, val_idx=val_idx)\n",
    "\n",
    "    return train_idx, val_idx\n",
    "\n",
    "\n",
    "train_targets = np.array(raw_train.targets)\n",
    "train_idx, val_idx = get_or_create_split_indices(\n",
    "    targets=train_targets,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    seed=SEED,\n",
    "    split_file=SPLIT_FILE,\n",
    ")\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=DATA_ROOT, train=True, download=False, transform=train_transform)\n",
    "val_dataset = datasets.FashionMNIST(root=DATA_ROOT, train=True, download=False, transform=eval_transform)\n",
    "test_dataset = datasets.FashionMNIST(root=DATA_ROOT, train=False, download=False, transform=eval_transform)\n",
    "\n",
    "train_subset = Subset(train_dataset, train_idx)\n",
    "val_subset = Subset(val_dataset, val_idx)\n",
    "\n",
    "print(f'Train subset size: {len(train_subset):,}')\n",
    "print(f'Validation subset size: {len(val_subset):,}')\n",
    "print(f'Test size: {len(test_dataset):,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_cache: dict[int, tuple[DataLoader, DataLoader]] = {}\n",
    "\n",
    "def make_train_val_loaders(batch_size: int) -> tuple[DataLoader, DataLoader]:\n",
    "    if batch_size in loader_cache:\n",
    "        return loader_cache[batch_size]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=NUM_WORKERS > 0,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=NUM_WORKERS > 0,\n",
    "    )\n",
    "    loader_cache[batch_size] = (train_loader, val_loader)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def make_test_loader(batch_size: int) -> DataLoader:\n",
    "    return DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=NUM_WORKERS > 0,\n",
    "    )\n",
    "\n",
    "example_loader, _ = make_train_val_loaders(batch_size=128)\n",
    "xb, yb = next(iter(example_loader))\n",
    "print('Batch X shape:', tuple(xb.shape))\n",
    "print('Batch y shape:', tuple(yb.shape))\n",
    "print('Batch dtype:', xb.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8b7dd",
   "metadata": {},
   "source": [
    "## CNN Architecture (Modern but Easy to Read)\n",
    "\n",
    "The model uses a compact **MBConv-style** design inspired by EfficientNet/MobileNet ideas:\n",
    "- depthwise separable convolutions (efficient)\n",
    "- squeeze-and-excitation blocks (channel attention)\n",
    "- residual connections when shapes match\n",
    "\n",
    "This is stronger than a basic CNN while still being understandable for first CNN practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5861bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcite(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 4):\n",
    "        super().__init__()\n",
    "        reduced = max(8, channels // reduction)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced, kernel_size=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(reduced, channels, kernel_size=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        scale = self.net(self.pool(x))\n",
    "        return x * scale\n",
    "\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        expand_ratio: int = 2,\n",
    "        stride: int = 1,\n",
    "        drop_rate: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_channels = int(in_channels * expand_ratio)\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.expand = nn.Identity()\n",
    "            depthwise_in = in_channels\n",
    "        else:\n",
    "            self.expand = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, hidden_channels, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_channels),\n",
    "                nn.SiLU(inplace=True),\n",
    "            )\n",
    "            depthwise_in = hidden_channels\n",
    "\n",
    "        self.depthwise = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                depthwise_in,\n",
    "                depthwise_in,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "                groups=depthwise_in,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(depthwise_in),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        self.se = SqueezeExcite(depthwise_in, reduction=4)\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(depthwise_in, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        self.dropout = nn.Dropout2d(drop_rate) if drop_rate > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "        x = self.expand(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.project(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.use_residual:\n",
    "            x = x + identity\n",
    "        return x\n",
    "\n",
    "\n",
    "class FashionEfficientCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10, dropout: float = 0.25):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        self.features = nn.Sequential(\n",
    "            MBConvBlock(32, 32, expand_ratio=2, stride=1),\n",
    "            MBConvBlock(32, 48, expand_ratio=2, stride=2),\n",
    "            MBConvBlock(48, 64, expand_ratio=2, stride=1),\n",
    "            MBConvBlock(64, 96, expand_ratio=2, stride=2),\n",
    "            MBConvBlock(96, 128, expand_ratio=2, stride=1),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(128, 192, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(192, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.stem(x)\n",
    "        x = self.features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "tmp_model = FashionEfficientCNN(num_classes=NUM_CLASSES, dropout=0.25).to(DEVICE)\n",
    "print(f'Trainable parameters: {count_trainable_parameters(tmp_model):,}')\n",
    "del tmp_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e247fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epoch_metrics(y_true: list[int], y_pred: list[int]) -> dict[str, float]:\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'macro_f1': f1_score(y_true, y_pred, average='macro'),\n",
    "    }\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    scaler: torch.cuda.amp.GradScaler,\n",
    ") -> dict[str, float]:\n",
    "    model.train()\n",
    "    losses = []\n",
    "    all_true, all_pred = [], []\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if AMP_ENABLED:\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        all_true.extend(labels.detach().cpu().tolist())\n",
    "        all_pred.extend(torch.argmax(logits.detach(), dim=1).cpu().tolist())\n",
    "\n",
    "    metrics = compute_epoch_metrics(all_true, all_pred)\n",
    "    metrics['loss'] = float(np.mean(losses))\n",
    "    return metrics\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    ") -> tuple[dict[str, float], list[int], list[int]]:\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_true, all_pred = [], []\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        if AMP_ENABLED:\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "        else:\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        all_true.extend(labels.detach().cpu().tolist())\n",
    "        all_pred.extend(torch.argmax(logits.detach(), dim=1).cpu().tolist())\n",
    "\n",
    "    metrics = compute_epoch_metrics(all_true, all_pred)\n",
    "    metrics['loss'] = float(np.mean(losses))\n",
    "    return metrics, all_true, all_pred\n",
    "\n",
    "\n",
    "def params_to_trial_id(params: dict[str, Any]) -> str:\n",
    "    payload = json.dumps(params, sort_keys=True)\n",
    "    return hashlib.sha1(payload.encode('utf-8')).hexdigest()[:12]\n",
    "\n",
    "\n",
    "def save_json(path: Path, payload: dict[str, Any]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(payload, indent=2))\n",
    "\n",
    "\n",
    "def run_training(\n",
    "    params: dict[str, Any],\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    max_epochs: int,\n",
    "    checkpoint_dir: Path | None = None,\n",
    "    trial: optuna.trial.Trial | None = None,\n",
    "    run_name: str = 'run',\n",
    ") -> dict[str, Any]:\n",
    "    model = FashionEfficientCNN(num_classes=NUM_CLASSES, dropout=params['dropout']).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=params['label_smoothing'])\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=params['lr'],\n",
    "        weight_decay=params['weight_decay'],\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=max_epochs,\n",
    "        eta_min=1e-6,\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=AMP_ENABLED)\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_val_macro_f1 = -1.0\n",
    "    best_state_dict = None\n",
    "    epochs_without_improvement = 0\n",
    "    history_rows: list[dict[str, float]] = []\n",
    "\n",
    "    last_ckpt_path = None\n",
    "    best_ckpt_path = None\n",
    "    history_csv_path = None\n",
    "    status_json_path = None\n",
    "\n",
    "    if checkpoint_dir is not None:\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        save_json(checkpoint_dir / 'params.json', params)\n",
    "\n",
    "        last_ckpt_path = checkpoint_dir / 'last.pt'\n",
    "        best_ckpt_path = checkpoint_dir / 'best.pt'\n",
    "        history_csv_path = checkpoint_dir / 'history.csv'\n",
    "        status_json_path = checkpoint_dir / 'status.json'\n",
    "\n",
    "        if status_json_path.exists():\n",
    "            status_data = json.loads(status_json_path.read_text())\n",
    "            if status_data.get('finished') and best_ckpt_path.exists():\n",
    "                best_payload = torch.load(best_ckpt_path, map_location=DEVICE)\n",
    "                model.load_state_dict(best_payload['model_state_dict'])\n",
    "                history_df = pd.read_csv(history_csv_path) if history_csv_path.exists() else pd.DataFrame()\n",
    "                return {\n",
    "                    'model': model,\n",
    "                    'history_df': history_df,\n",
    "                    'best_val_macro_f1': status_data['best_val_macro_f1'],\n",
    "                    'epochs_ran': int(status_data['epochs_ran']),\n",
    "                }\n",
    "\n",
    "        if last_ckpt_path.exists():\n",
    "            payload = torch.load(last_ckpt_path, map_location=DEVICE)\n",
    "            model.load_state_dict(payload['model_state_dict'])\n",
    "            optimizer.load_state_dict(payload['optimizer_state_dict'])\n",
    "            scheduler.load_state_dict(payload['scheduler_state_dict'])\n",
    "            if payload.get('scaler_state_dict') is not None:\n",
    "                scaler.load_state_dict(payload['scaler_state_dict'])\n",
    "            start_epoch = int(payload['epoch']) + 1\n",
    "            best_val_macro_f1 = float(payload['best_val_macro_f1'])\n",
    "            epochs_without_improvement = int(payload['epochs_without_improvement'])\n",
    "            history_rows = payload.get('history_rows', [])\n",
    "\n",
    "    if start_epoch >= max_epochs:\n",
    "        if best_state_dict is None and best_ckpt_path is not None and best_ckpt_path.exists():\n",
    "            best_payload = torch.load(best_ckpt_path, map_location=DEVICE)\n",
    "            model.load_state_dict(best_payload['model_state_dict'])\n",
    "        history_df = pd.DataFrame(history_rows)\n",
    "        return {\n",
    "            'model': model,\n",
    "            'history_df': history_df,\n",
    "            'best_val_macro_f1': best_val_macro_f1,\n",
    "            'epochs_ran': len(history_rows),\n",
    "        }\n",
    "\n",
    "    for epoch in range(start_epoch, max_epochs):\n",
    "        train_metrics = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n",
    "        val_metrics, _, _ = evaluate_one_epoch(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        row = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_metrics['loss'],\n",
    "            'train_accuracy': train_metrics['accuracy'],\n",
    "            'train_macro_f1': train_metrics['macro_f1'],\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_accuracy': val_metrics['accuracy'],\n",
    "            'val_macro_f1': val_metrics['macro_f1'],\n",
    "            'lr': float(optimizer.param_groups[0]['lr']),\n",
    "        }\n",
    "        history_rows.append(row)\n",
    "\n",
    "        val_score = val_metrics['macro_f1']\n",
    "        improved = val_score > (best_val_macro_f1 + 1e-4)\n",
    "        if improved:\n",
    "            best_val_macro_f1 = val_score\n",
    "            best_state_dict = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if checkpoint_dir is not None:\n",
    "            payload = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict() if AMP_ENABLED else None,\n",
    "                'best_val_macro_f1': best_val_macro_f1,\n",
    "                'epochs_without_improvement': epochs_without_improvement,\n",
    "                'history_rows': history_rows,\n",
    "            }\n",
    "            torch.save(payload, last_ckpt_path)\n",
    "\n",
    "            if improved:\n",
    "                torch.save({'model_state_dict': model.state_dict()}, best_ckpt_path)\n",
    "\n",
    "            pd.DataFrame(history_rows).to_csv(history_csv_path, index=False)\n",
    "            save_json(\n",
    "                status_json_path,\n",
    "                {\n",
    "                    'finished': False,\n",
    "                    'best_val_macro_f1': best_val_macro_f1,\n",
    "                    'epochs_ran': len(history_rows),\n",
    "                },\n",
    "            )\n",
    "\n",
    "        if trial is not None:\n",
    "            trial.report(val_score, step=epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned(f'Pruned at epoch {epoch}')\n",
    "\n",
    "        if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "            break\n",
    "\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "    elif best_ckpt_path is not None and best_ckpt_path.exists():\n",
    "        best_payload = torch.load(best_ckpt_path, map_location=DEVICE)\n",
    "        model.load_state_dict(best_payload['model_state_dict'])\n",
    "\n",
    "    history_df = pd.DataFrame(history_rows)\n",
    "\n",
    "    if checkpoint_dir is not None and status_json_path is not None:\n",
    "        save_json(\n",
    "            status_json_path,\n",
    "            {\n",
    "                'finished': True,\n",
    "                'best_val_macro_f1': float(best_val_macro_f1),\n",
    "                'epochs_ran': int(len(history_rows)),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'history_df': history_df,\n",
    "        'best_val_macro_f1': float(best_val_macro_f1),\n",
    "        'epochs_ran': int(len(history_rows)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a3d68",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Optuna\n",
    "\n",
    "We tune learning and regularization hyperparameters:\n",
    "- learning rate\n",
    "- weight decay\n",
    "- dropout\n",
    "- label smoothing\n",
    "- batch size\n",
    "\n",
    "Optimization target: **validation Macro-F1** (good class-balanced metric).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1202cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_params(trial: optuna.trial.Trial) -> dict[str, Any]:\n",
    "    return {\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 5e-3, log=True),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 5e-3, log=True),\n",
    "        'dropout': trial.suggest_float('dropout', 0.10, 0.45),\n",
    "        'label_smoothing': trial.suggest_float('label_smoothing', 0.0, 0.12),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),\n",
    "    }\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    params = suggest_params(trial)\n",
    "    train_loader, val_loader = make_train_val_loaders(batch_size=params['batch_size'])\n",
    "\n",
    "    checkpoint_dir = None\n",
    "    trial_id = params_to_trial_id(params)\n",
    "    if ENABLE_CHECKPOINTING:\n",
    "        checkpoint_dir = ARTIFACT_DIR / 'trials' / trial_id\n",
    "\n",
    "    result = run_training(\n",
    "        params=params,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        trial=trial,\n",
    "        run_name=f'trial_{trial.number:03d}',\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr('trial_id', trial_id)\n",
    "    trial.set_user_attr('epochs_ran', result['epochs_ran'])\n",
    "    if checkpoint_dir is not None:\n",
    "        trial.set_user_attr('checkpoint_dir', str(checkpoint_dir))\n",
    "\n",
    "    return result['best_val_macro_f1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n",
    "storage = optuna.storages.RDBStorage(\n",
    "    url=STUDY_STORAGE,\n",
    "    heartbeat_interval=60,\n",
    "    grace_period=180,\n",
    "    failed_trial_callback=optuna.storages.RetryFailedTrialCallback(max_retry=2),\n",
    ")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=storage,\n",
    "    load_if_exists=True,\n",
    "    direction='maximize',\n",
    "    sampler=sampler,\n",
    "    pruner=pruner,\n",
    ")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=OPTUNA_TIMEOUT, show_progress_bar=True)\n",
    "search_minutes = (time.perf_counter() - start_time) / 60\n",
    "\n",
    "print(f'Finished/loaded {len(study.trials)} trials in {search_minutes:.2f} minutes this session.')\n",
    "print('Best validation Macro-F1:', round(study.best_value, 4))\n",
    "print('Best params:', study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df = study.trials_dataframe()\n",
    "display(trials_df.sort_values('value', ascending=False).head(10))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "completed = trials_df[trials_df['state'] == 'COMPLETE']\n",
    "plt.plot(completed['number'], completed['value'], marker='o')\n",
    "plt.title('Validation Macro-F1 by completed trial')\n",
    "plt.xlabel('Trial number')\n",
    "plt.ylabel('Macro-F1')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f5670",
   "metadata": {},
   "source": [
    "## Final Training and Out-of-Sample Test Metrics\n",
    "\n",
    "After tuning, we train one final model using the best hyperparameters and evaluate on the held-out test set.\n",
    "\n",
    "Reported metrics:\n",
    "- test loss\n",
    "- test accuracy\n",
    "- test Macro-F1\n",
    "- per-class classification report\n",
    "- confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e88dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params.copy()\n",
    "train_loader, val_loader = make_train_val_loaders(batch_size=best_params['batch_size'])\n",
    "\n",
    "final_checkpoint_dir = None\n",
    "if ENABLE_CHECKPOINTING:\n",
    "    final_checkpoint_dir = ARTIFACT_DIR / 'final_model'\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "final_result = run_training(\n",
    "    params=best_params,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_epochs=FINAL_MAX_EPOCHS,\n",
    "    checkpoint_dir=final_checkpoint_dir,\n",
    "    trial=None,\n",
    "    run_name='final_model',\n",
    ")\n",
    "final_minutes = (time.perf_counter() - start_time) / 60\n",
    "\n",
    "best_model = final_result['model']\n",
    "print(f\"Final training finished in {final_minutes:.2f} minutes\")\n",
    "print('Best validation Macro-F1:', round(final_result['best_val_macro_f1'], 4))\n",
    "print('Epochs used:', final_result['epochs_ran'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4553b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_eval = nn.CrossEntropyLoss()\n",
    "test_loader = make_test_loader(batch_size=best_params['batch_size'])\n",
    "test_metrics, y_true_test, y_pred_test = evaluate_one_epoch(best_model, test_loader, criterion_eval)\n",
    "\n",
    "print('Test metrics (out-of-sample):')\n",
    "print(f\"  Loss:      {test_metrics['loss']:.4f}\")\n",
    "print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Macro-F1:  {test_metrics['macro_f1']:.4f}\")\n",
    "\n",
    "print('\n",
    "Per-class report:')\n",
    "print(classification_report(y_true_test, y_pred_test, target_names=CLASS_NAMES, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680eaf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=CLASS_NAMES,\n",
    "    yticklabels=CLASS_NAMES,\n",
    ")\n",
    "plt.title('Test confusion matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_CHECKPOINTING:\n",
    "    summary = {\n",
    "        'study_name': STUDY_NAME,\n",
    "        'best_trial_number': int(study.best_trial.number),\n",
    "        'best_validation_macro_f1': float(study.best_value),\n",
    "        'best_params': study.best_params,\n",
    "        'test_metrics': {\n",
    "            'loss': float(test_metrics['loss']),\n",
    "            'accuracy': float(test_metrics['accuracy']),\n",
    "            'macro_f1': float(test_metrics['macro_f1']),\n",
    "        },\n",
    "    }\n",
    "    summary_path = ARTIFACT_DIR / 'final_summary.json'\n",
    "    summary_path.write_text(json.dumps(summary, indent=2))\n",
    "    print('Saved summary to', summary_path.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
