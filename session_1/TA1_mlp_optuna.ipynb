{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with Optuna Tuning\n",
    "\n",
    "This notebook replaces the LightGBM model with a PyTorch Multi-Layer Perceptron (MLP).\n",
    "It implements a strict Cross-Validation pipeline where `TargetEncoder` and `CountVectorizer` are fit ONLY on the training folds to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1cf10311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import KNNImputer\n",
    "# from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "# from category_encoders import TargetEncoder\n",
    "from torchmetrics.functional.classification import binary_auroc as roc_auc, binary_average_precision as pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fac70693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setting up the same seed as in the replication notebook.\n",
    "\"\"\"\n",
    "SEED = 3508706438\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9e52fab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: MPS\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using device: MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using device: CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01967385",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Feature Engineering (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f04e5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "test_hef = pd.read_csv('../data/MIMIC/MIMIC_III_dataset_death/mimic_test_death.csv')\n",
    "train_hef = pd.read_csv('../data/MIMIC/MIMIC_III_dataset_death/mimic_train.csv')\n",
    "extra_diag = pd.read_csv('../data/MIMIC/MIMIC_III_dataset_death/extra_data/MIMIC_diagnoses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8c3e0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_diag['ICD9_CODE'] = extra_diag['ICD9_CODE'].astype(str)\n",
    "extra_diag['ICD9_CHAPTER'] = extra_diag['ICD9_CODE'].str[:3]\n",
    "\n",
    "extra_diag['IS_SEPSIS'] = extra_diag['ICD9_CODE'].str.startswith(('9959', '7855')).astype(int)\n",
    "extra_diag['IS_HEART_FAIL'] = extra_diag['ICD9_CODE'].str.startswith('428').astype(int)\n",
    "extra_diag['IS_CANCER'] = extra_diag['ICD9_CODE'].str.startswith(('196', '197', '198', '199')).astype(int)\n",
    "extra_diag['IS_RENAL'] = extra_diag['ICD9_CODE'].str.startswith(('584', '585')).astype(int)\n",
    "\n",
    "diag_grouped = extra_diag.groupby('HADM_ID').agg({\n",
    "    'ICD9_CODE': [\n",
    "        ('NUM_DIAGNOSES', 'count'),                                  \n",
    "        ('DIAG_STRING', lambda x: ' '.join(x.dropna().astype(str)))],\n",
    "    'ICD9_CHAPTER': [('UNIQUE_CHAPTERS', 'nunique')],\n",
    "    'IS_SEPSIS': [('HAS_SEPSIS', 'max')],\n",
    "    'IS_HEART_FAIL': [('HAS_HEART_FAIL', 'max')],\n",
    "    'IS_CANCER': [('HAS_CANCER', 'max')],\n",
    "    'IS_RENAL': [('HAS_RENAL', 'max')]\n",
    "})\n",
    "\n",
    "diag_grouped.columns = diag_grouped.columns.droplevel(0)\n",
    "diag_grouped = diag_grouped.reset_index()\n",
    "\n",
    "# Merge features\n",
    "train_hef = train_hef.merge(diag_grouped, left_on='hadm_id', right_on='HADM_ID', how='left')\n",
    "test_hef = test_hef.merge(diag_grouped, left_on='hadm_id', right_on='HADM_ID', how='left')\n",
    "\n",
    "# Drop HADM_ID as it's not needed anymore\n",
    "train_hef.drop('HADM_ID', axis=1, inplace=True)\n",
    "test_hef.drop('HADM_ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a1a4b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df_input):\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    df['ADMITTIME'] = pd.to_datetime(df['ADMITTIME'])\n",
    "    df['DOB'] = pd.to_datetime(df['DOB'])\n",
    "    df['AGE'] = df['ADMITTIME'].dt.year - df['DOB'].dt.year\n",
    "    df.loc[df['AGE'] > 89, 'AGE'] = 90\n",
    "    df.loc[df['AGE'] < 0, 'AGE'] = df['AGE'].median()\n",
    "    \n",
    "    original_index = df.index\n",
    "    df = df.sort_values(by=['subject_id', 'ADMITTIME'])\n",
    "    df['PREV_ICU_STAYS'] = df.groupby('subject_id').cumcount()\n",
    "    df['LAST_ADMIT'] = df.groupby('subject_id')['ADMITTIME'].shift(1)\n",
    "    seconds_diff = (df['ADMITTIME'] - df['LAST_ADMIT']).dt.total_seconds()\n",
    "    df['DAYS_SINCE_LAST'] = seconds_diff / (24 * 3600)\n",
    "    df['DAYS_SINCE_LAST'] = df['DAYS_SINCE_LAST'].fillna(-1)\n",
    "    df = df.reindex(original_index)\n",
    "\n",
    "    cols_to_drop = ['ADMITTIME', 'DOB', 'LAST_ADMIT', 'DISCHTIME', 'DEATHTIME', \n",
    "                    'DOD', 'LOS', 'Diff', 'MeanBP_Min', 'MeanBP_Max', \n",
    "                    'MeanBP_Mean', 'subject_id' #, 'hadm_id'\n",
    "                    ]\n",
    "    df = df.drop([c for c in cols_to_drop if c in df.columns], axis=1)\n",
    "    return df\n",
    "\n",
    "train_processed = engineer_features(train_hef)\n",
    "test_processed = engineer_features(test_hef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "49cdf704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Column Groups\n",
    "num_cols = [\n",
    "    'HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean', \n",
    "    'SysBP_Min', 'SysBP_Max', 'SysBP_Mean', \n",
    "    'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean', \n",
    "    'RespRate_Min', 'RespRate_Max', 'RespRate_Mean', \n",
    "    'TempC_Min', 'TempC_Max', 'TempC_Mean', \n",
    "    'SpO2_Min', 'SpO2_Max', 'SpO2_Mean', \n",
    "    'Glucose_Min', 'Glucose_Max', 'Glucose_Mean', \n",
    "    'PREV_ICU_STAYS', 'AGE', 'DAYS_SINCE_LAST', \n",
    "    'NUM_DIAGNOSES', 'UNIQUE_CHAPTERS', \n",
    "    'HAS_SEPSIS', 'HAS_HEART_FAIL', 'HAS_CANCER', 'HAS_RENAL'] \n",
    "\n",
    "categorical_cols = [\n",
    "    'ICD9_diagnosis', 'DIAGNOSIS', 'FIRST_CAREUNIT', \n",
    "    'GENDER', 'ADMISSION_TYPE', 'INSURANCE', 'RELIGION', 'MARITAL_STATUS', 'ETHNICITY']\n",
    "\n",
    "text_col = 'DIAG_STRING'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c2bd9",
   "metadata": {},
   "source": [
    "## 2. Define a Dynamic MLP Model\n",
    "Simply a model that flexibly creates an MLP with the desired number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5893d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicMLP(nn.Module):\n",
    "    def __init__(self, input_dim, n_layers, hidden_dim, dropout_rate):\n",
    "        super(DynamicMLP, self).__init__()\n",
    "        layers = []\n",
    "        curr_dim = input_dim\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(curr_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            curr_dim = hidden_dim\n",
    "        layers.append(nn.Linear(curr_dim, 1))\n",
    "        \n",
    "        # Note: We do NOT use Sigmoid here because we use BCEWithLogitsLoss for numerical stability\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581dd653",
   "metadata": {},
   "source": [
    "## 3. Pre-computing CV Splits (To Prevent Leakage)\n",
    "We iterate over 5 folds. In each fold, we fit `TargetEncoder` and `CountVectorizer` **only** on the training index, and then transform the validation index (but we do this in the optuna study to tune `CountVectorizer`'s `max_features` as a parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3c939bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV setup complete. Text vectorization moved to Optuna study for tuning.\n"
     ]
    }
   ],
   "source": [
    "# Prepare for Cross Validation\n",
    "X = train_processed.drop('HOSPITAL_EXPIRE_FLAG', axis=1).copy()\n",
    "y = train_processed['HOSPITAL_EXPIRE_FLAG'].values.copy()\n",
    "\n",
    "# StratifiedGroupKFold to prevent leakage in medical data groups\n",
    "skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "print(\"CV setup complete. Text vectorization moved to Optuna study for tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c890a7e",
   "metadata": {},
   "source": [
    "## 4. Optuna Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "17b0c33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20885])\n",
      "torch.Size([20885, 1])\n"
     ]
    }
   ],
   "source": [
    "y_tr_t = torch.tensor(y, dtype=torch.float32)\n",
    "print(y_tr_t.shape)\n",
    "\n",
    "y_tr_t = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "print(y_tr_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2693b314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-13 20:41:35,135] A new study created in RDB with name: 20260213_20:41_mimic_model_optimization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3598b1d8ea9743279694b71902b48e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-02-13 20:42:32,637] Trial 0 finished with value: 0.6724823832511901 and parameters: {'max_features': 200, 'n_layers': 7, 'hidden_dim': 224, 'dropout_rate': 0.23685369060038014, 'lr': 0.0001173594457988223, 'pos_weight': 5.905664413684116, 'batch_size': 8192}. Best is trial 0 with value: 0.6724823832511901.\n",
      "[I 2026-02-13 20:43:36,785] Trial 1 finished with value: 0.8610927939414978 and parameters: {'max_features': 800, 'n_layers': 6, 'hidden_dim': 256, 'dropout_rate': 0.18376775895297676, 'lr': 0.0002892313593910923, 'pos_weight': 5.820197856165914, 'batch_size': 2048}. Best is trial 1 with value: 0.8610927939414978.\n",
      "[I 2026-02-13 20:44:35,090] Trial 2 finished with value: 0.6645766496658325 and parameters: {'max_features': 200, 'n_layers': 5, 'hidden_dim': 256, 'dropout_rate': 0.3918536349256135, 'lr': 0.0001341717374227959, 'pos_weight': 3.662581381696976, 'batch_size': 20885}. Best is trial 1 with value: 0.8610927939414978.\n",
      "[I 2026-02-13 20:45:27,949] Trial 3 finished with value: 0.8589829444885254 and parameters: {'max_features': 900, 'n_layers': 1, 'hidden_dim': 448, 'dropout_rate': 0.276147224371716, 'lr': 0.0006668706877499002, 'pos_weight': 4.250559017857711, 'batch_size': 1024}. Best is trial 1 with value: 0.8610927939414978.\n",
      "[I 2026-02-13 20:46:23,860] Trial 4 finished with value: 0.8634516835212708 and parameters: {'max_features': 1000, 'n_layers': 2, 'hidden_dim': 224, 'dropout_rate': 0.12214785722125468, 'lr': 0.0008120600356644543, 'pos_weight': 3.900988257999951, 'batch_size': 1024}. Best is trial 4 with value: 0.8634516835212708.\n",
      "\n",
      "Best trial params: {'max_features': 1000, 'n_layers': 2, 'hidden_dim': 224, 'dropout_rate': 0.12214785722125468, 'lr': 0.0008120600356644543, 'pos_weight': 3.900988257999951, 'batch_size': 1024}\n",
      "\n",
      "Best score: 0.86\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # --- Hyperparameters ---\n",
    "    max_features = trial.suggest_int(\"max_features\", 200, 1000, step=100)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 7)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 512, step=32)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.01, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    # HEF class distribution: 0.89 neg, 0.11 pos.\n",
    "    # To perfectly balance it, we need to multiply the weight of the positives by 8.1 approx.\n",
    "    pos_weight_val = trial.suggest_float(\"pos_weight\", 1.0, 8.5)\n",
    "    \n",
    "    # We are using large batch sizes because our data fits perfectly in memory.\n",
    "    size_train_data = len(train_processed)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [1024, 2048, 4096, 8192, size_train_data])\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y, groups=train_processed['hadm_id'])):\n",
    "        X_tr_df, X_val_df = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        X_tr_df_num = imputer.fit_transform(X_tr_df[num_cols])\n",
    "        X_val_df_num = imputer.transform(X_val_df[num_cols])\n",
    "        \n",
    "        # 1. Numerical scaling\n",
    "        X_tr_num = torch.tensor(X_tr_df_num, dtype=torch.float32)\n",
    "        X_val_num = torch.tensor(X_val_df_num, dtype=torch.float32)\n",
    "        mean, std = X_tr_num.mean(0), X_tr_num.std(0)\n",
    "        X_tr_num = (X_tr_num - mean) / (std + 1e-7)\n",
    "        X_val_num = (X_val_num - mean) / (std + 1e-7)\n",
    "        \n",
    "        # 2. Text Vectorization\n",
    "        vectorizer = CountVectorizer(binary=True, token_pattern=r'(?u)\\b\\w+\\b', max_features=max_features)\n",
    "        X_tr_text = torch.tensor(vectorizer.fit_transform(X_tr_df[text_col]).todense(), dtype=torch.float32)\n",
    "        X_val_text = torch.tensor(vectorizer.transform(X_val_df[text_col]).todense(), dtype=torch.float32)\n",
    "        \n",
    "        # # 2.1 Categorical: Target Encode\n",
    "        # encoder = TargetEncoder(cols=categorical_cols)\n",
    "        # X_tr_cat = torch.tensor(encoder.fit_transform(X_tr_df[categorical_cols], y_tr).values, dtype=torch.float32)\n",
    "        # X_val_cat = torch.tensor(encoder.transform(X_val_df[categorical_cols]).values, dtype=torch.float32)\n",
    "        \n",
    "        # 3. Concatenate\n",
    "        X_tr_final = torch.cat([X_tr_num, X_tr_text], dim=1)\n",
    "        X_val_final = torch.cat([X_val_num, X_val_text], dim=1)\n",
    "        # X_tr_final = torch.cat([X_tr_num, X_tr_cat, X_tr_text], dim=1) # Uncomment for categorical\n",
    "        \n",
    "        y_tr_t = torch.tensor(y_tr, dtype=torch.float32).unsqueeze(1)\n",
    "        y_val_t = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(X_tr_final, y_tr_t), batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        model = DynamicMLP(X_tr_final.shape[1], n_layers, hidden_dim, dropout_rate).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight_val]).to(device))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        best_fold_score = 0\n",
    "        patience = 20\n",
    "        no_impr = 0\n",
    "        \n",
    "        # Training Loop with Early Stopping\n",
    "        for epoch in range(10):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(xb), yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(X_val_final.to(device))\n",
    "                probs = torch.sigmoid(logits).flatten()\n",
    "                labels = y_val_t.to(device).int().flatten()\n",
    "                \n",
    "                cur_roc = roc_auc(probs, labels)\n",
    "                cur_pr = pr_auc(probs, labels)\n",
    "                \n",
    "                score = (0.75 * cur_roc + 0.25 * cur_pr).item()\n",
    "                \n",
    "            if score > best_fold_score:\n",
    "                best_fold_score = score\n",
    "                no_impr = 0\n",
    "            else:\n",
    "                no_impr += 1\n",
    "                \n",
    "            if no_impr >= patience:\n",
    "                break\n",
    "        \n",
    "        fold_scores.append(best_fold_score)\n",
    "        \n",
    "    return np.mean(fold_scores)\n",
    "\n",
    "# Create Study\n",
    "db_path = Path(\"../data/MIMIC/optuna_HEF/optuna_mimic.db\").resolve()\n",
    "date_minutes = datetime.now().strftime(\"%Y%m%d_%H:%M\")\n",
    "study = optuna.create_study(\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    direction=\"maximize\",\n",
    "    storage=f\"sqlite:///{db_path.as_posix()}\",\n",
    "    study_name=f\"{date_minutes}_mimic_model_optimization\",\n",
    "    load_if_exists=True\n",
    "    )\n",
    "study.optimize(objective, n_trials=5, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nBest trial params:\", study.best_params)\n",
    "print(f\"\\nBest score: {study.best_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1bd6cb",
   "metadata": {},
   "source": [
    "## 5. Final Training on Full Data\n",
    "Now we retrain the best model on the entire training set (using the same leakage-prevention pipeline for the final test set transformation) and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b63fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prediction to ../data/MIMIC/pytorch_mlp_optuna_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare Full Training Data\n",
    "X_train_df = train_processed.drop('HOSPITAL_EXPIRE_FLAG', axis=1).copy()\n",
    "y_train_full = train_processed['HOSPITAL_EXPIRE_FLAG'].values.copy()\n",
    "X_test_df = test_processed.copy()\n",
    "\n",
    "best_params = study.best_params\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_tr_df_num = imputer.fit_transform(X_train_df[num_cols])\n",
    "X_te_df_num = imputer.transform(X_test_df[num_cols])\n",
    "\n",
    "# Numerical\n",
    "X_tr_num = torch.tensor(X_tr_df_num, dtype=torch.float32)\n",
    "X_te_num = torch.tensor(X_te_df_num, dtype=torch.float32)\n",
    "mean, std = X_tr_num.mean(0), X_tr_num.std(0)\n",
    "X_tr_num = (X_tr_num - mean) / (std + 1e-7)\n",
    "X_te_num = (X_te_num - mean) / (std + 1e-7)\n",
    "\n",
    "# # Categorical\n",
    "# encoder = TargetEncoder(cols=categorical_cols)\n",
    "# X_tr_cat = torch.tensor(encoder.fit_transform(X_train_df[categorical_cols], y_train_full).values, dtype=torch.float32)\n",
    "# X_te_cat = torch.tensor(encoder.transform(X_test_df[categorical_cols]).values, dtype=torch.float32)\n",
    "\n",
    "# Text\n",
    "vectorizer = CountVectorizer(binary=True, token_pattern=r'(?u)\\b\\w+\\b', max_features=best_params['max_features'])\n",
    "X_tr_text = torch.tensor(vectorizer.fit_transform(X_train_df[text_col]).todense(), dtype=torch.float32)\n",
    "X_te_text = torch.tensor(vectorizer.transform(X_test_df[text_col]).todense(), dtype=torch.float32)\n",
    "\n",
    "# Concatenate\n",
    "X_train_final = torch.cat([X_tr_num, X_tr_text], dim=1)\n",
    "X_test_final = torch.cat([X_te_num, X_te_text], dim=1)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_full, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# 2. Final Training\n",
    "final_model = DynamicMLP(\n",
    "                    X_train_final.shape[1],\n",
    "                    best_params['n_layers'], \n",
    "                    best_params['hidden_dim'],\n",
    "                    best_params['dropout_rate']\n",
    "                ).to(device)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([best_params['pos_weight']]).to(device))\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_final, y_train_tensor), \n",
    "                            batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "final_model.train()\n",
    "for epoch in range(50):\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(final_model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 3. Predict\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = final_model(X_test_final.to(device))\n",
    "    y_proba = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "\n",
    "# Save\n",
    "submission = pd.DataFrame({'icustay_id': test_hef['icustay_id'], 'prediction': y_proba})\n",
    "submission.to_csv(f'../data/MIMIC/{date_minutes}_pytorch_mlp_optuna_submission.csv', index=False)\n",
    "print(\"Saved prediction to ../data/MIMIC/pytorch_mlp_optuna_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta-sessions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
