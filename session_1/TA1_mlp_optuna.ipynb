{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with Optuna Tuning\n",
    "\n",
    "This notebook replaces the LightGBM model with a PyTorch Multi-Layer Perceptron (MLP).\n",
    "It implements a strict Cross-Validation pipeline where `TargetEncoder` and `CountVectorizer` are fit ONLY on the training folds to prevent data leakage.\n",
    "\n",
    "### Strategy for Efficiency in the Cross-Validation Loop:\n",
    "1. **Pre-computation**: We generate the 5 CV splits *before* starting the Optuna study.\n",
    "2. **Leakage Prevention**: For each split, we fit encoders on the Training set, transform the Validation set, and store the resulting Tensors.\n",
    "3. **Optuna**: The objective function simply loads these pre-prepared tensors, making the tuning loop very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac70693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setting up the same seed as in the replication notebook.\n",
    "\"\"\"\n",
    "SEED = 3508706438\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Feature Engineering (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04e5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "test_hef = pd.read_csv('../data/MIMIC/MIMIC_III_dataset_death/mimic_test_death.csv')\n",
    "train_hef = pd.read_csv('../data/MIMIC/MIMIC_III_dataset_death/mimic_train.csv')\n",
    "extra_diag = pd.read_csv('../data/MIMIC/MIMIC_III_dataset_death/extra_data/MIMIC_diagnoses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_diag['ICD9_CODE'] = extra_diag['ICD9_CODE'].astype(str)\n",
    "extra_diag['ICD9_CHAPTER'] = extra_diag['ICD9_CODE'].str[:3]\n",
    "\n",
    "extra_diag['IS_SEPSIS'] = extra_diag['ICD9_CODE'].str.startswith(('9959', '7855')).astype(int)\n",
    "extra_diag['IS_HEART_FAIL'] = extra_diag['ICD9_CODE'].str.startswith('428').astype(int)\n",
    "extra_diag['IS_CANCER'] = extra_diag['ICD9_CODE'].str.startswith(('196', '197', '198', '199')).astype(int)\n",
    "extra_diag['IS_RENAL'] = extra_diag['ICD9_CODE'].str.startswith(('584', '585')).astype(int)\n",
    "\n",
    "diag_grouped = extra_diag.groupby('HADM_ID').agg({\n",
    "    'ICD9_CODE': [\n",
    "        ('NUM_DIAGNOSES', 'count'),                                  \n",
    "        ('DIAG_STRING', lambda x: ' '.join(x.dropna().astype(str)))],\n",
    "    'ICD9_CHAPTER': [('UNIQUE_CHAPTERS', 'nunique')],\n",
    "    'IS_SEPSIS': [('HAS_SEPSIS', 'max')],\n",
    "    'IS_HEART_FAIL': [('HAS_HEART_FAIL', 'max')],\n",
    "    'IS_CANCER': [('HAS_CANCER', 'max')],\n",
    "    'IS_RENAL': [('HAS_RENAL', 'max')]\n",
    "})\n",
    "\n",
    "diag_grouped.columns = diag_grouped.columns.droplevel(0)\n",
    "diag_grouped = diag_grouped.reset_index()\n",
    "\n",
    "# Merge features\n",
    "train_hef = train_hef.merge(diag_grouped, left_on='hadm_id', right_on='HADM_ID', how='left')\n",
    "test_hef = test_hef.merge(diag_grouped, left_on='hadm_id', right_on='HADM_ID', how='left')\n",
    "\n",
    "# Drop HADM_ID as it's not needed anymore\n",
    "train_hef.drop('HADM_ID', axis=1, inplace=True)\n",
    "test_hef.drop('HADM_ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df_input):\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    df['ADMITTIME'] = pd.to_datetime(df['ADMITTIME'])\n",
    "    df['DOB'] = pd.to_datetime(df['DOB'])\n",
    "    df['AGE'] = df['ADMITTIME'].dt.year - df['DOB'].dt.year\n",
    "    df.loc[df['AGE'] > 89, 'AGE'] = 90\n",
    "    df.loc[df['AGE'] < 0, 'AGE'] = df['AGE'].median()\n",
    "    \n",
    "    original_index = df.index\n",
    "    df = df.sort_values(by=['subject_id', 'ADMITTIME'])\n",
    "    df['PREV_ICU_STAYS'] = df.groupby('subject_id').cumcount()\n",
    "    df['LAST_ADMIT'] = df.groupby('subject_id')['ADMITTIME'].shift(1)\n",
    "    seconds_diff = (df['ADMITTIME'] - df['LAST_ADMIT']).dt.total_seconds()\n",
    "    df['DAYS_SINCE_LAST'] = seconds_diff / (24 * 3600)\n",
    "    df['DAYS_SINCE_LAST'] = df['DAYS_SINCE_LAST'].fillna(-1)\n",
    "    df = df.reindex(original_index)\n",
    "\n",
    "    cols_to_drop = ['ADMITTIME', 'DOB', 'LAST_ADMIT', 'DISCHTIME', 'DEATHTIME', \n",
    "                    'DOD', 'LOS', 'Diff', 'MeanBP_Min', 'MeanBP_Max', \n",
    "                    'MeanBP_Mean', 'hadm_id', 'subject_id']\n",
    "    df = df.drop([c for c in cols_to_drop if c in df.columns], axis=1)\n",
    "    return df\n",
    "\n",
    "train_processed = engineer_features(train_hef)\n",
    "test_processed = engineer_features(test_hef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Column Groups\n",
    "num_cols = [\n",
    "    'HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean', \n",
    "    'SysBP_Min', 'SysBP_Max', 'SysBP_Mean', \n",
    "    'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean', \n",
    "    'RespRate_Min', 'RespRate_Max', 'RespRate_Mean', \n",
    "    'TempC_Min', 'TempC_Max', 'TempC_Mean', \n",
    "    'SpO2_Min', 'SpO2_Max', 'SpO2_Mean', \n",
    "    'Glucose_Min', 'Glucose_Max', 'Glucose_Mean', \n",
    "    'PREV_ICU_STAYS', 'AGE', 'DAYS_SINCE_LAST', \n",
    "    'NUM_DIAGNOSES', 'UNIQUE_CHAPTERS', \n",
    "    'HAS_SEPSIS', 'HAS_HEART_FAIL', 'HAS_CANCER', 'HAS_RENAL'] \n",
    "\n",
    "categorical_cols = [\n",
    "    'ICD9_diagnosis', 'DIAGNOSIS', 'FIRST_CAREUNIT', \n",
    "    'GENDER', 'ADMISSION_TYPE', 'INSURANCE', 'RELIGION', 'MARITAL_STATUS', 'ETHNICITY']\n",
    "\n",
    "text_col = 'DIAG_STRING'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-computing CV Splits (To Prevent Leakage)\n",
    "We iterate over 5 folds. In each fold, we fit `TargetEncoder` and `CountVectorizer` **only** on the training index, and then transform the validation index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for Cross Validation\n",
    "X = train_processed.drop('HOSPITAL_EXPIRE_FLAG', axis=1)\n",
    "y = train_processed['HOSPITAL_EXPIRE_FLAG'].values\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Store processed tensors for each fold\n",
    "cv_datasets = []\n",
    "\n",
    "print(\"Pre-processing CV folds...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    # Split DataFrames\n",
    "    X_tr_df, X_val_df = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # 1. Numerical: Standard Scale (Fit on Train, Transform Val)\n",
    "    X_tr_num = torch.tensor(X_tr_df[num_cols].values, dtype=torch.float32)\n",
    "    X_val_num = torch.tensor(X_val_df[num_cols].values, dtype=torch.float32)\n",
    "    \n",
    "    mean = X_tr_num.mean(dim=0)\n",
    "    std = X_tr_num.std(dim=0)\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    X_tr_num = (X_tr_num - mean) / (std + epsilon)\n",
    "    X_val_num = (X_val_num - mean) / (std + epsilon)\n",
    "    \n",
    "    # # 2. Categorical: Target Encode (Fit on Train, Transform Val)\n",
    "    # encoder = TargetEncoder(cols=categorical_cols)\n",
    "    # # Note: TargetEncoder needs y to fit\n",
    "    # X_tr_cat = encoder.fit_transform(X_tr_df[categorical_cols], y_tr)\n",
    "    # X_val_cat = encoder.transform(X_val_df[categorical_cols])\n",
    "    \n",
    "    # X_tr_cat = torch.tensor(X_tr_cat.values, dtype=torch.float32)\n",
    "    # X_val_cat = torch.tensor(X_val_cat.values, dtype=torch.float32)\n",
    "    \n",
    "    # 3. Text: CountVectorizer (Fit on Train, Transform Val)\n",
    "    vect_max_features = 800\n",
    "    vectorizer = CountVectorizer(binary=True, token_pattern=r'(?u)\\b\\w+\\b', max_features=vect_max_features)\n",
    "    \n",
    "    X_tr_text_sparse = vectorizer.fit_transform(X_tr_df[text_col])\n",
    "    X_val_text_sparse = vectorizer.transform(X_val_df[text_col])\n",
    "    \n",
    "    X_tr_text = torch.tensor(X_tr_text_sparse.todense(), dtype=torch.float32)\n",
    "    X_val_text = torch.tensor(X_val_text_sparse.todense(), dtype=torch.float32)\n",
    "    \n",
    "    # 4. Concatenate\n",
    "    X_tr_final = torch.cat([X_tr_num, X_tr_text], dim=1)\n",
    "    X_val_final = torch.cat([X_val_num, X_val_text], dim=1)\n",
    "    \n",
    "    y_tr_tensor = torch.tensor(y_tr, dtype=torch.float32).unsqueeze(1)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    # Store in list\n",
    "    cv_datasets.append({\n",
    "        'train': TensorDataset(X_tr_final, y_tr_tensor),\n",
    "        'val': TensorDataset(X_val_final, y_val_tensor),\n",
    "        'input_dim': X_tr_final.shape[1]\n",
    "    })\n",
    "    \n",
    "    print(f\"Fold {fold+1} prepared. Input shape: {X_tr_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optuna Hyperparameter Tuning\n",
    "We optimize: Hidden Dimension, Dropout Rate, Learning Rate, and Batch Size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    epochs = 100 # Better not to overdo epochs for tuning speed\n",
    "\n",
    "    fold_aurocs = []\n",
    "    \n",
    "    # Iterate through pre-prepared folds\n",
    "    for fold_data in cv_datasets:\n",
    "        train_loader = DataLoader(fold_data['train'], batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(fold_data['val'], batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        model = SimpleMLP(fold_data['input_dim'], hidden_dim, dropout_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        # Training Loop\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Validation Loop\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                preds = model(X_batch)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        try:\n",
    "            auroc = roc_auc_score(all_labels, all_preds)\n",
    "        except ValueError:\n",
    "            auroc = 0.5 # Handle edge cases\n",
    "            \n",
    "        fold_aurocs.append(auroc)\n",
    "    \n",
    "    return np.mean(fold_aurocs)\n",
    "\n",
    "# Create Study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Training on Full Data\n",
    "Now we retrain the best model on the entire training set (using the same leakage-prevention pipeline for the final test set transformation) and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare Full Training Data\n",
    "X_train_df = train_processed.drop('HOSPITAL_EXPIRE_FLAG', axis=1)\n",
    "y_train_full = train_processed['HOSPITAL_EXPIRE_FLAG'].values\n",
    "X_test_df = test_processed.copy()\n",
    "\n",
    "# --- Process Full Train ---\n",
    "# Numerical\n",
    "X_tr_num = torch.tensor(X_train_df[num_cols].values, dtype=torch.float32)\n",
    "X_te_num = torch.tensor(X_test_df[num_cols].values, dtype=torch.float32)\n",
    "\n",
    "mean = X_tr_num.mean(dim=0)\n",
    "std = X_tr_num.std(dim=0)\n",
    "X_tr_num = (X_tr_num - mean) / (std + 1e-7)\n",
    "X_te_num = (X_te_num - mean) / (std + 1e-7)\n",
    "\n",
    "# # Categorical\n",
    "# encoder = TargetEncoder(cols=categorical_cols)\n",
    "# X_tr_cat = encoder.fit_transform(X_train_df[categorical_cols], y_train_full)\n",
    "# X_te_cat = encoder.transform(X_test_df[categorical_cols])\n",
    "# X_tr_cat = torch.tensor(X_tr_cat.values, dtype=torch.float32)\n",
    "# X_te_cat = torch.tensor(X_te_cat.values, dtype=torch.float32)\n",
    "\n",
    "# Text\n",
    "vectorizer = CountVectorizer(binary=True, token_pattern=r'(?u)\\b\\w+\\b', max_features=800)\n",
    "X_tr_text_sparse = vectorizer.fit_transform(X_train_df[text_col])\n",
    "X_te_text_sparse = vectorizer.transform(X_test_df[text_col])\n",
    "X_tr_text = torch.tensor(X_tr_text_sparse.todense(), dtype=torch.float32)\n",
    "X_te_text = torch.tensor(X_te_text_sparse.todense(), dtype=torch.float32)\n",
    "\n",
    "# Final Tensors\n",
    "X_train_final = torch.cat([X_tr_num, X_tr_text], dim=1)\n",
    "X_test_final = torch.cat([X_te_num, X_te_text], dim=1)\n",
    "y_train_tensor = torch.tensor(y_train_full, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# 2. Train Best Model\n",
    "best_params = study.best_params\n",
    "final_model = SimpleMLP(X_train_final.shape[1], best_params['hidden_dim'], best_params['dropout_rate'])\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_final, y_train_tensor), \n",
    "                            batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "final_model.train()\n",
    "for epoch in range(15): # Train a bit longer for final model\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 3. Predict\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_proba = final_model(X_test_final).numpy().flatten()\n",
    "\n",
    "# Save\n",
    "submission = pd.DataFrame({'icustay_id': test_hef['icustay_id'], 'prediction': y_proba})\n",
    "submission.to_csv('../data/MIMIC/pytorch_mlp_optuna_submission.csv', index=False)\n",
    "print(\"Saved prediction to ../data/MIMIC/pytorch_mlp_optuna_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
